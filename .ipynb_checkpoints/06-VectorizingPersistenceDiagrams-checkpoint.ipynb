{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Persistence Diagrams\n",
    "\n",
    "In this notebook, we'll analyze a synthetic dataset and a real dataset using vectorized TDA signatures. Specifically, we'll turn persistence diagrams into persistence images and use these as features in a standard machine learning pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages for linear algebra and statistics\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import a package to time computations\n",
    "import time\n",
    "\n",
    "# Import specialized TDA packages\n",
    "from ripser import Rips\n",
    "from persim import PersImage\n",
    "\n",
    "# Package for including a figure in the notebook\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an expanded version of the demonstration available at https://github.com/scikit-tda/persim. I added more parameters to the experiment and more explanation for everything.\n",
    "\n",
    "This notebook shows how you can use persistent homology and persistence images to classify datasets.  We construct datasets from two classes, one just noise and the other noise with a big circle in the middle. We then compute persistence diagrams with [Ripser.py](https://github.com/scikit-tda/ripser.py) and convert them to persistence images with [PersIm](https://github.com/scikit-tda/persim). Using these persistence images, we build classifier using Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the dataset\n",
    "\n",
    "We will construct a data set consisting of several pointclouds which are just noise and several pointclouds which are noise on top of a noisy circle. The goal is to see whether tools from persistent homology can be used to classify the data into two categories. \n",
    "\n",
    "This is synthetic data, but one can imagine realistic tasks along these lines; e.g. https://en.wikipedia.org/wiki/Transmission_electron_cryomicroscopy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cryo EM](data/Cryoem_groel.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining a function which generates a noisy point cloud sampled from a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spherical(npoints, scale=1, ndim=2):\n",
    "    vec = np.random.randn(ndim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    vec = scale*np.transpose(vec)\n",
    "    return vec\n",
    "\n",
    "def noisy_sphere(npoints, scale=1, noiseLevel=0.3, offset = 0, ndim=2):\n",
    "    \"\"\"\n",
    "    npoints = number of points sampled\n",
    "    scale = radius of the sphere being sampled\n",
    "    noiseLevel = how \"noisy\" the samples are\n",
    "    offset = shifts the circle in the (1,1,...,1) direction\n",
    "    \"\"\"\n",
    "    data = sample_spherical(npoints,scale, ndim)+scale*noiseLevel*np.random.random((npoints,ndim)) + offset*np.ones(ndim)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = noisy_sphere(200, scale=20, offset=30)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(data[:, 0], data[:, 1], 'ob');\n",
    "ax1.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function which produces a noisy point cloud in space, with a given scale, in a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(N, scale, ndim=2):\n",
    "    return scale * np.random.random((N, 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct our data set. There are lots of parameters to change to produce different experiments. In general we generate samples of random noise and samples of random noise with noisy circles embedded in them. We can change numbers of samples, how noisy everything is, size and location of the circles, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first define a bunch of parameters for the experiment\n",
    "total_samples = 200 # Number of samples in our experiment\n",
    "samples_per_class = int(total_samples / 2) \n",
    "# Number of samples which are just noise. We will do our experiment taking an equal number each type of sample.\n",
    "npoints = 400\n",
    "# Number of points in each point cloud sample.\n",
    "circle_noise_level = 0.4 # Noise level for the circles.\n",
    "snr = 0.25\n",
    "# Signal-to-noise ratio. For the images containing circles, this is the percentage of points that belong to the circle.\n",
    "noise_scale = 150 # How large the noisy point cloud is\n",
    "circle_scale = 30 # How large the noisy circle is. Should be chosen relative to the noise_scale.\n",
    "\n",
    "# Generate the pure noise samples\n",
    "just_noise = [noise(npoints, noise_scale) for _ in range(samples_per_class)]\n",
    "\n",
    "# Generate the noise+circle samples\n",
    "with_circle = [np.concatenate((noisy_sphere(int(snr*npoints), noiseLevel = circle_noise_level, scale=circle_scale, offset=(0.2*np.random.random()+0.4)*noise_scale), noise(npoints-int(snr*npoints), noise_scale)))\n",
    "               for _ in range(samples_per_class)]\n",
    "\n",
    "# Combine all the samples into one list.\n",
    "datas = []\n",
    "datas.extend(just_noise)\n",
    "datas.extend(with_circle)\n",
    "\n",
    "# Define labels. Pure noise samples are labeled with 0, noise+circles are labeled with 1.\n",
    "# These labels will be used to train/test our classifyer.\n",
    "labels = np.zeros(total_samples)\n",
    "labels[samples_per_class:] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data. We'll plot a random pure noise sample and a random circle+noise sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "random_sample_choice = np.random.randint(100) # Pick a number between 0 and 100.\n",
    "\n",
    "xs, ys = just_noise[random_sample_choice][:,0], just_noise[random_sample_choice][:,1]\n",
    "axs[0].scatter(xs, ys)\n",
    "axs[0].set_title(\"Example noise dataset\")\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "\n",
    "xs_, ys_ = with_circle[random_sample_choice][:,0], with_circle[random_sample_choice][:,1]\n",
    "axs[1].scatter(xs_, ys_)\n",
    "axs[1].set_title(\"Example noise with circle dataset\")\n",
    "axs[1].set_aspect('equal', 'box')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistent homology of each dataset\n",
    "\n",
    "Generate the persistence diagram of $H_1$ for each of the datasets generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rips = Rips(maxdim=1, coeff=2); # Apply ripser with Z_2 coefficients, up to dimension 1.\n",
    "diagrams = [rips.fit_transform(data) for data in datas]\n",
    "diagrams_h1 = [rips.fit_transform(data)[1] for data in datas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot persistence diagrams for a random choice of noise sample and a random choice of circle+noise. We'll use the \"lifetime\" plot style option because this is a step in the process of creating persistence images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "\n",
    "random_sample = np.random.randint(50)\n",
    "\n",
    "rips.plot(diagrams_h1[random_sample], show=False, legend=False, lifetime=True)\n",
    "plt.title(\"PD of $H_1$ for just noise\")\n",
    "\n",
    "plt.subplot(122)\n",
    "rips.plot(diagrams_h1[-random_sample], show=False, legend=False, lifetime=True)\n",
    "plt.title(\"PD of $H_1$ for circle w/ noise\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistence images\n",
    "\n",
    "We will discuss the definition of a persistence image in class. The 'persim' package is made to handle persistence images. Recall from class that we need to choose a resolution and a variance for the Gaussians. The weight function is automatically chosen to be linearly increasing in the $y$-direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pim = PersImage(pixels=(20,20), spread=1)\n",
    "imgs = pim.transform(diagrams_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the persistence images. We should interpret each image as the pixelated plot of a function $\\mathbb{R}^2 \\rightarrow \\mathbb{R}$, with colors corresponding to height of the plot. Such a plot can be understood as a vector: by reshaping the 20-pixel-by-20-pixel image into a list of 400 numbers, we can think of it as a vector in $\\mathbb{R}^{400}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(240+i+1)\n",
    "    pim.show(imgs[i], ax)\n",
    "    plt.title(\"PI of $H_1$ for noise\")\n",
    "\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(240+i+5)\n",
    "    pim.show(imgs[-(i+1)], ax)\n",
    "    plt.title(\"PI of $H_1$ for circle w/ noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the datasets from the persistence images\n",
    "\n",
    "Now we'll train a classifier on the vectorized persistence diagrams. We first flatten each persistence image so that it is really represented as a vector in $\\mathbb{R}^{400}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_array = np.array([img.flatten() for img in imgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we randomly divide our data into a training set and a testing set (with labels for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imgs_array, labels, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a Support Vector Machines classifier on the training set. Roughly, SVM looks for a $399$-dimensional affine hyperplane in $\\mathbb{R}^{400}$ which best separates the two classes in the training data set. This hyperplane can then be used to guess the label of unseen data from the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVM = SVC(kernel=\"linear\") # Defines the model with optional parameters\n",
    "\n",
    "modelSVM.fit(X_train, y_train) # Trains the model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the trained classifier on the unseen testing data to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVM.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs extremely well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperplane we found in the training stage is determined by a normal vector (plus a bias). This is a $400$-dimensional vector, which we can reshape back into an image to get an idea of how the classifier distinguishes the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifying_image = np.copy(modelSVM.coef_).reshape((20,20))\n",
    "plt.imshow(classifying_image,cmap = plt.cm.RdBu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to a  Naive Baseline\n",
    "\n",
    "To test whether this performance is a result of the efficacy of logistic regression, or the persistence image representation is actually useful, let's try to classify with a more naive approach.\n",
    "\n",
    "Each sample in our data set is an npoints-by-2 array. We could flatten each of these to get a vector in $\\mathbb{R}^{2 \\cdot \\mbox{npoints}}$, then run the same regression procedure to see if logistic regression can classify these vectors. \n",
    "\n",
    "First we flatten the data, split into training and test sets and perform regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_array = np.array([sample.flatten() for sample in datas])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(datas_array, labels, test_size=0.40, random_state=42)\n",
    "\n",
    "modelSVM = SVC(kernel=\"linear\") # Defines the model with optional parameters\n",
    "modelSVM.fit(X_train, y_train) # Trains the model on the training data\n",
    "\n",
    "print('Naive SVM score:', modelSVM.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least the result is better than random guessing...\n",
    "\n",
    "We can visualize the classifying vector learned by logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifying_image = np.copy(modelSVM.coef_).reshape((400,2))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(classifying_image[:, 0], classifying_image[:, 1],'ob')\n",
    "ax1.axis('equal'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Weighted Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is also simulated, but comes from a more realistic source. The data comes from a set of ~400 *districting plans* for the state of Pennsylvania---each districting plan is a partition of Pennsylvania into 18 (path connected) districts, each of which has the same population (up to a small tolerance). \n",
    "\n",
    "To get a persistence diagram from a districting plan:\n",
    "- The districting plan is converted into an adjacency graph for the districts\n",
    "- nodes are weighted by, say, Republican vote share in a given election\n",
    "- each edge is assigned the maximum value of its neighboring nodes, so that we get an overall filtration of the graph (considered as a simplicial complex)\n",
    "- sublevel set degree-0 persistent homology is run on the resulting filtered simplicial complex\n",
    "\n",
    "This is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/districting.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying statistics of the space of all districting plans is an important part of treating the *gerrymandering problem*, which is the problem that districts can be drawn adversarily to favor one party over the other, regardless of the vote distribution in the state. In the paper https://arxiv.org/abs/2007.02390, we use these TDA signatures as proxies for districting plans in order to simplify statistical analyses. \n",
    "\n",
    "Unfortunately, the space of districting plans is extremely large and complicated. Ensembles of districting plans are generated via various Markov chain algorithms which can be used to explore the space of potential plans.\n",
    "\n",
    "In this dataset, we have generated plans in two classes. Each class used a Markov chain which aimed to maximize the number of districts one by either Republicans or Democrats in a given election. Our task is to see if these plans can be classified by their topological signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data\n",
    "\n",
    "The persistence diagrams for the weighted graphs are precomputed and saved. So all we have to do is load in the persistence features from the csv files in the data folder. The data is stored in csv files with the most persistent feature for all plans stored in the first file, second most persistent feature in another file, etc. The classes are a bit unbalanced here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes = []\n",
    "\n",
    "for j in range(6):\n",
    "    fileName = \"data/50biased_\"+str(j)+\".csv\"\n",
    "    file = np.genfromtxt(fileName,delimiter=',')\n",
    "    barcodes.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the various classes into barcodes for the democrat-biased plans..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_barcodes = []\n",
    "\n",
    "for j in range(1,243):\n",
    "    barcode = []\n",
    "    for k in range(6):\n",
    "        bar = barcodes[k][j,1:3]\n",
    "        barcode.append(bar)\n",
    "    dem_barcodes.append(np.array(barcode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the republican-biased plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_barcodes = []\n",
    "\n",
    "for j in range(243,431):\n",
    "    barcode = []\n",
    "    for k in range(6):\n",
    "        bar = barcodes[k][j,1:3]\n",
    "        barcode.append(bar)\n",
    "    rep_barcodes.append(np.array(barcode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine them into one big list of barcodes and record the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_barcodes = dem_barcodes + rep_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0]*len(dem_barcodes)+[1]*len(rep_barcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create persistence images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "\n",
    "random_sample = np.random.randint(50)\n",
    "\n",
    "rips.plot(all_barcodes[random_sample], show=False, legend=False, lifetime=True)\n",
    "plt.title(\"PD of Democratic Plan\")\n",
    "\n",
    "plt.subplot(122)\n",
    "rips.plot(all_barcodes[-random_sample], show=False, legend=False, lifetime=True)\n",
    "plt.title(\"PD Republican Plan\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim = PersImage(pixels=[30,30], spread=0.01)\n",
    "imgs = pim.transform(all_barcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the persistence images as pixelated heat maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(240+i+1)\n",
    "    pim.show(imgs[20+i], ax)\n",
    "    plt.title(\"PI of Democrat Plan\")\n",
    "\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(240+i+5)\n",
    "    pim.show(imgs[-(i+20)], ax)\n",
    "    plt.title(\"PI of Republican Plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "As before, we will flatten the images into 1d arrays (vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_array = np.array([img.flatten() for img in imgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imgs_array, y, test_size=0.3, random_state=1, stratify = y)\n",
    "print('Testing set size:',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train and test a SVM model, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVM = SVC(kernel=\"linear\")\n",
    "modelSVM.fit(X_train, y_train)\n",
    "\n",
    "modelSVM.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly subtle classification problem, but the TDA signatures seem to do a good job of capturing the differences in these weighted graphs. Changing the `random_state` variable above gives different results, but one can get a more robust statistic by running this over several splits of the data and averaging.\n",
    "\n",
    "This can be improved with some feature tuning. For example, observe that the most persistent point is pretty much the same across all plans---this is Philadelphia. Removing highly persistent points from all diagrams actually has a positive effect on classification for this example. \n",
    "\n",
    "**Unofficial HW:** See if you can improve this classification rate with some tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we can reshape the coefficients of the SVM classification vector to get an idea of how classification works here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifying_image = modelSVM.coef_.reshape((30,30))\n",
    "plt.imshow(classifying_image, cmap = plt.cm.RdBu)\n",
    "plt.colorbar();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
