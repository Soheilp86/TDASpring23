{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takens Embeddings\n",
    "\n",
    "In this notebook, we'll explore the Takens embeddings introduced in lecture for applying TDA methods to time series data.\n",
    "\n",
    "We'll use tge package `giotto-tda` which has a Takens embedding module with a lot of nice features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gtda modules\n",
    "from gtda.time_series import SingleTakensEmbedding\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import Scaler, Filtering, PersistenceEntropy, BettiCurve, PairwiseDistance\n",
    "from gtda.graphs import KNeighborsGraph, GraphGeodesicDistance\n",
    "from gtda.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# gtda plotting functions\n",
    "from gtda.plotting import plot_heatmap\n",
    "\n",
    "# Import data from openml\n",
    "import openml\n",
    "from openml.datasets.functions import get_dataset\n",
    "\n",
    "# Plotting functions\n",
    "from gtda.plotting import plot_diagram, plot_betti_surfaces\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# MatPLotLib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "We'll start with a simple signal consisting of a noisy sine curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "domain = np.linspace(0,2*np.pi,n_samples)\n",
    "noise_level = 0.5\n",
    "frequency = 4\n",
    "\n",
    "signal = np.array([np.sin(frequency*x) + noise_level * np.random.random() for x in domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(domain,signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sliding window embedding* of the signal $f:[0,2\\pi] \\to \\mathbb{R}$ is defined in terms of two parameters:\n",
    "- $\\tau$ is the *delay*\n",
    "- $d$ is the *embedding dimension*\n",
    "\n",
    "The sliding window embedding is given by\n",
    "\\begin{align}\n",
    "\\mathrm{SW}_{d,\\tau}f:[0,2\\pi] &\\to \\mathbb{R}^d \\\\\n",
    "t &\\mapsto \\mathrm{SW}_{d,\\tau}f(t) = (f(t),f(t + \\tau),f(t+2\\tau),\\ldots,f(t+(d-1)\\tau))\n",
    "\\end{align}\n",
    "\n",
    "Let's start by embedding in $\\mathbb{R}^3$ (i.e. $d = 3$). We can experiment with different delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 3\n",
    "time_delay = 10\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type='fixed',dimension=dimension,time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of embedded point cloud:', embedded_signal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of points in the point cloud is `n_samples`-$(d-1)\\tau$. This is to mitigate boundary effects on the sliding window.\n",
    "\n",
    "Since we embedded in $\\mathbb{R}^3$, we can take a look at the embedded point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xs = embedded_signal[:,0]\n",
    "ys = embedded_signal[:,1]\n",
    "zs = embedded_signal[:,2]\n",
    "\n",
    "ax.scatter(xs, ys, zs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a noisy ellipse, which agrees with our computation in lecture. `giotto` has some useful plotting tools which allow us to manipulate the plot to get a better view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point_cloud(embedded_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did a computation in class which showed that the embedded point cloud is at its 'roundest' when $\\omega \\cdot d \\cdot \\tau = 0 \\, (\\mathrm{mod} \\, \\pi)$, where $\\omega$ is the frequency. Since we are embedding with $d = 3$, we should get a round circle when $\\tau = \\frac{\\pi}{3\\omega}$. Let's test this out.\n",
    "\n",
    "**Note:** Since we are working with a discrete approximation of the signal, $\\tau$ is input as an integer---i.e., it's the number of indices to skip in the vector of signal sample values. This means we need to do a coordinate transformation and use the following time delay\n",
    "$$\n",
    "\\hat{\\tau} = \\tau \\cdot \\frac{\\mathrm{n}}{2\\pi} = \\frac{\\pi}{3\\omega} \\cdot \\frac{\\mathrm{n}}{2\\pi} = \\frac{\\mathrm{n}}{6 \\omega},\n",
    "$$\n",
    "where $n = $`n_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delay = int(n_samples/(6*frequency))\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type='fixed',dimension=dimension,time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)\n",
    "\n",
    "plot_point_cloud(embedded_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying this for different values of $\\tau$, we see that the value above does seem to be of 'optimal roundness'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.pi/5 # tau is a number between 0 and 2pi\n",
    "\n",
    "time_delay = int(tau*n_samples/(2*np.pi)) # coordinate transformation to discrete steps\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type='fixed',dimension=dimension,time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)\n",
    "\n",
    "plot_point_cloud(embedded_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch back to $\\tau = \\frac{\\pi}{3\\omega}$. \n",
    "\n",
    "Next we observe that this circle is actually 'multiply covered'. I.e., since the signal runs through $\\omega$ full periods, the circle is run over $\\omega$ times.\n",
    "\n",
    "To see this, we can plot subsets of the embedded point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.pi/(3*frequency) \n",
    "time_delay = int(tau*n_samples/(2*np.pi))\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type='fixed',dimension=dimension,time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)\n",
    "\n",
    "plot_start = 0*int(1000/frequency)\n",
    "plot_end = 1*int(1000/frequency)\n",
    "\n",
    "truncated_embedded_signal = embedded_signal[plot_start:plot_end,:]\n",
    "plot_point_cloud(truncated_embedded_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projecting the point cloud to one of the coordinate axes produces a 1-dimensional signal which looks like what we started with, up to a translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_coord = 0\n",
    "projected_signal = embedded_signal[:,projection_coord]\n",
    "plt.plot(projected_signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling Takens' Theorem: the 'philosophical idea' here is that the embedded circle represents a bunch of steps in some dynamical system, which lie along an attractor. The projected 1-dimensional signal represents the output of some observation function. The sliding window embedding then reconstructs the attractor! The reconstruction is only up to topology---observe that for different choices of $\\tau$, we get different geometries, but the same topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the PH of the embedded point cloud, we see that it has the (noisy) homology of a circle. The following is the syntax used for persistent homology in the `giotto` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_dimensions = [1]\n",
    "VR = VietorisRipsPersistence(\n",
    "    metric='euclidean', max_edge_length=100, homology_dimensions=homology_dimensions)\n",
    "\n",
    "X_diagrams = VR.fit_transform(embedded_signal[None, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(X_diagrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 'distinctness' of the degree-1 feature depends a lot on the parameters for the Takens embedding. We can see this by running over a few choices of $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taus = [np.pi/(10*frequency),np.pi/(5*frequency),np.pi/(3*frequency),np.pi/(2*frequency)]\n",
    "\n",
    "for tau in taus: \n",
    "    time_delay = int(tau*n_samples/(2*np.pi))\n",
    "\n",
    "    embedder = SingleTakensEmbedding(parameters_type='fixed',dimension=dimension,time_delay=time_delay)\n",
    "    embedded_signal = embedder.fit_transform(signal)\n",
    "    \n",
    "    X_diagrams = VR.fit_transform(embedded_signal[None, :, :])\n",
    "    plot_diagram(X_diagrams[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should lead you to ask: given a signal with unknown 'ground truth' topology, how do we choose the correct $\\tau$? Furthermore, how do we choose the correct embedding dimension $d$?\n",
    "\n",
    "To my knowledge, this is still an open theoretical question in the TDA literature. The `giotto` package does have some statistical heuristics to choose 'optimal' parameters. To use this functionality, the user initializes with upper bounds on $d$ and $\\tau$, and some existing algorithms for phase space reconstruction are used to adjust them.\n",
    "\n",
    "**Note:** The initial upper bounds must satisfy $n \\geq (d-1)*\\tau$, where $n$ is the number of samples and $\\tau$ is the discrete time delay.\n",
    "\n",
    "**Heuristic:** Since the initializations give upper bounds, choosing them high (but satisfying the inequality above) seems to work best. However, choosing high upper bounds also increases compute time, so there is a trade off here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 30\n",
    "time_delay = 30\n",
    "\n",
    "embedder = SingleTakensEmbedding(parameters_type='search',dimension=dimension,time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)\n",
    "\n",
    "print('Optimal embedding time delay based on mutual information:', embedder.time_delay_)\n",
    "print('Optimal embedding dimension based on false nearest neighbors:',embedder.dimension_)\n",
    "print('Shape of embedded point cloud:', embedded_signal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_dimensions = [1]\n",
    "VR = VietorisRipsPersistence(\n",
    "    metric='euclidean', max_edge_length=100, homology_dimensions=homology_dimensions)\n",
    "\n",
    "X_diagrams = VR.fit_transform(embedded_signal[None, :, :])\n",
    "plot_diagram(X_diagrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're still using the original paramters for the signal, `n_samples=1000` and `frequency = 4`, then using the upper bounds `dimension`=`time_delay`=`30` gives 'optimal parameters' $d = 8$ and $\\tau = 28$. Converting the discrete time delay to a continuous one, we get\n",
    "$$\n",
    "\\tau = 28 \\cdot \\frac{2\\pi}{1000}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\omega \\cdot \\tau \\cdot d  = 4 \\cdot 28 \\cdot \\frac{2\\pi}{1000} \\cdot 8 = \\frac{1792}{1000}\\pi \\approx 1.8\\pi,\n",
    "$$\n",
    "so we *almost* get something equal to zero, mod $\\pi$; i.e., we almost get an optimally round circle through this parameter search.\n",
    "\n",
    "**(Unofficial) Homework:** Run this experiment many times, using different frequencies and record the values that you get for $\\omega \\cdot \\tau \\cdot d$. Does the opimization algorithm tend to produce $d$ and $\\tau$ which make this product close to a multiple of $\\pi$? If the answer is \"Yes\", that's great! If the answer is \"No\", maybe there is some room for improvement in this optimization algorithm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorenz Attractor\n",
    "\n",
    "We motivated Takens' Theorem with the Lorenz system. Let's take a closer look at this using the `giotto` package. We can load in a point cloud version of the attractor as a preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = get_dataset(42182).get_data(dataset_format='array')[0]\n",
    "plot_point_cloud(point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project to one of the axes to get a time series, serving as a proxy for the values of an observation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = point_cloud[:,0]\n",
    "plt.plot(signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting time series is much more complicated than our sine curve above. Let's see if the sliding window embedding reconstructs the Lorenz attractor (topologically).\n",
    "\n",
    "**Full Disclosure:** The upper bound of 3 on the time delay in the next cell was chosen by playing with parameters for a while. So this reconstruction is not fully unsupervised..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 20\n",
    "time_delay = 3\n",
    "embedder = SingleTakensEmbedding(parameters_type='search', dimension=embedding_dimension, time_delay=time_delay)\n",
    "embedded_signal = embedder.fit_transform(signal)\n",
    "\n",
    "print('Optimal embedding time delay based on mutual information:', embedder.time_delay_)\n",
    "print('Optimal embedding dimension based on false nearest neighbors:',embedder.dimension_)\n",
    "print('Shape of embedded point cloud:', embedded_signal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are embedding in high dimensions, we can't plot the full dimensional point cloud. We can project to a 3-dimensional subspace and plot the result. We'll use the first three principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X = pca.fit_transform(embedded_signal)\n",
    "plot_point_cloud(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the persistent homology of a point cloud with ~5k points takes a while. Let's subsample the point cloud before doing the computation, for the sake of class time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_factor = 5\n",
    "subsampled_embedded_signal = np.array([embedded_signal[subsample_factor*t,:] for t in range(int(len(embedded_signal)/subsample_factor))])\n",
    "print(len(subsampled_embedded_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homology_dimensions = [1]\n",
    "VR = VietorisRipsPersistence(\n",
    "    metric='euclidean', max_edge_length=100, homology_dimensions=homology_dimensions)\n",
    "\n",
    "X_diagrams = VR.fit_transform(subsampled_embedded_signal[None, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagram(X_diagrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gravitational Waves\n",
    "\n",
    "Now let's use Takens embedding in a more realistic data analysis pipeline. This part of the notebook is adapted from an example provided in the `giotto-tda` documentation --- see [GitHub](https://github.com/giotto-ai/giotto-tda/blob/master/examples/gravitational_waves_detection.ipynb). They have plenty of other cool examples to try freely available on their GitHub, I highly recommend checking it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment attepts to detect gravitational waves from noisy data using Takens embeddings. It is based on the article [Detection of gravitational waves using topological data analysis and convolutional neural network: An improved approach](https://arxiv.org/abs/1910.08245) by Christopher Bresten and Jae-Hun Jung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the article, the authors create a synthetic training set as follows: \n",
    "\n",
    "* Generate gravitational wave signals that correspond to non-spinning binary black hole mergers\n",
    "* Generate a noisy time series and embed a gravitational wave signal with probability 0.5 at a random time.\n",
    "\n",
    "The result is a set of time series of the form\n",
    "\n",
    "$$ s = g + \\epsilon \\frac{1}{R}\\xi $$\n",
    "\n",
    "where $g$ is a gravitational wave signal from the reference set, $\\xi$ is Gaussian noise, $\\epsilon=10^{-19}$ scales the noise amplitude to the signal, and $R \\in (0.075, 0.65)$ is a parameter that controls the signal-to-noise-ratio (SNR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant signal-to-noise ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warmup, let's generate some noisy signals with a constant SNR of 17.98. As shown in Table 1 of the article, this corresponds to an $R$ value of 0.65. We'll run the experiment on a small-ish dataset for the sake of finishing within class time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def make_gravitational_waves(\n",
    "    path_to_data: Path,\n",
    "    n_signals: int = 30,\n",
    "    downsample_factor: int = 2,\n",
    "    r_min: float = 0.075,\n",
    "    r_max: float = 0.65,\n",
    "    n_snr_values: int = 10,\n",
    "        ):\n",
    "    def padrand(V, n, kr):\n",
    "        cut = np.random.randint(n)\n",
    "        rand1 = np.random.randn(cut)\n",
    "        rand2 = np.random.randn(n - cut)\n",
    "        out = np.concatenate((rand1 * kr, V, rand2 * kr))\n",
    "        return out\n",
    "\n",
    "    Rcoef = np.linspace(r_min, r_max, n_snr_values)\n",
    "    Npad = 500  # number of padding points on either side of the vector\n",
    "    gw = np.load(path_to_data / \"gravitational_wave_signals.npy\")\n",
    "    Norig = len(gw[\"data\"][0])\n",
    "    Ndat = len(gw[\"signal_present\"])\n",
    "    N = int(Norig / downsample_factor)\n",
    "\n",
    "    ncoeff = []\n",
    "    Rcoeflist = []\n",
    "\n",
    "    for j in range(n_signals):\n",
    "        ncoeff.append(10 ** (-19) * (1 / Rcoef[j % n_snr_values]))\n",
    "        Rcoeflist.append(Rcoef[j % n_snr_values])\n",
    "\n",
    "    noisy_signals = []\n",
    "    gw_signals = []\n",
    "    k = 0\n",
    "    labels = np.zeros(n_signals)\n",
    "\n",
    "    for j in range(n_signals):\n",
    "        signal = gw[\"data\"][j % Ndat][range(0, Norig, downsample_factor)]\n",
    "        sigp = int((np.random.randn() < 0))\n",
    "        noise = ncoeff[j] * np.random.randn(N)\n",
    "        labels[j] = sigp\n",
    "        if sigp == 1:\n",
    "            rawsig = padrand(signal + noise, Npad, ncoeff[j])\n",
    "            if k == 0:\n",
    "                k = 1\n",
    "        else:\n",
    "            rawsig = padrand(noise, Npad, ncoeff[j])\n",
    "        noisy_signals.append(rawsig.copy())\n",
    "        gw_signals.append(signal)\n",
    "\n",
    "    return noisy_signals, gw_signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0.65\n",
    "n_signals = 100\n",
    "DATA = Path(\"./data\")\n",
    "\n",
    "noisy_signals, gw_signals, labels = make_gravitational_waves(\n",
    "    path_to_data=DATA, n_signals=n_signals, r_min=R, r_max=R, n_snr_values=1\n",
    ")\n",
    "\n",
    "print(f\"Number of noisy signals: {len(noisy_signals)}\")\n",
    "print(f\"Number of timesteps per series: {len(noisy_signals[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# get the index corresponding to the first pure noise time series\n",
    "background_idx = np.argmin(labels)\n",
    "# get the index corresponding to the first noise + gravitational wave time series\n",
    "signal_idx = np.argmax(labels)\n",
    "\n",
    "ts_noise = noisy_signals[background_idx]\n",
    "ts_background = noisy_signals[signal_idx]\n",
    "ts_signal = gw_signals[signal_idx]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_noise))), y=ts_noise, mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(ts_background))),\n",
    "        y=ts_background,\n",
    "        mode=\"lines\",\n",
    "        name=\"background\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(ts_signal))), y=ts_signal, mode=\"lines\", name=\"signal\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Takens embedding to turn each time series in our dataset into a point cloud. Note that there is an extra parameter here called 'stride'. This has the effect of subsampling the time series (by a factor of `stride`). This will be useful to improve computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 30\n",
    "embedding_time_delay = 30\n",
    "stride = 5\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=6, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_gw_embedded = embedder.fit_transform(gw_signals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did above, we can project to a 3-dimensional PCA basis to visualize the embedded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "y_gw_embedded_pca = pca.fit_transform(y_gw_embedded)\n",
    "\n",
    "plot_point_cloud(y_gw_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualizes an embedding of pure noise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 30\n",
    "embedding_time_delay = 30\n",
    "stride = 5\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=6, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_noise_embedded = embedder.fit_transform(noisy_signals[background_idx])\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "y_noise_embedded_pca = pca.fit_transform(y_noise_embedded)\n",
    "\n",
    "plot_point_cloud(y_noise_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the topological feature generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a Takens embedding for each time series in our dataset, normalizes the embedded point clouds, computes degree-0 and degree-1 persistent homology and then collects some summary statistics---in particular, the [Persistence Entropy](https://arxiv.org/pdf/1902.06467.pdf). Each time series ends up being represented by a two-dimensional vector!\n",
    "\n",
    "The hyperparameters below are taken from the original example on the `giotto` GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy, Scaler\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.metaestimators import CollectionTransformer\n",
    "from gtda.pipeline import Pipeline\n",
    "from gtda.time_series import TakensEmbedding\n",
    "\n",
    "embedding_dimension = 200\n",
    "embedding_time_delay = 10\n",
    "stride = 10\n",
    "\n",
    "embedder = TakensEmbedding(time_delay=embedding_time_delay,\n",
    "                           dimension=embedding_dimension,\n",
    "                           stride=stride)\n",
    "\n",
    "batch_pca = CollectionTransformer(PCA(n_components=3))\n",
    "\n",
    "persistence = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "\n",
    "scaling = Scaler()\n",
    "\n",
    "entropy = PersistenceEntropy(normalize=True, nan_fill_value=-10)\n",
    "\n",
    "\n",
    "steps = [(\"embedder\", embedder),\n",
    "         (\"pca\", batch_pca),\n",
    "         (\"persistence\", persistence),\n",
    "         (\"scaling\", scaling),\n",
    "         (\"entropy\", entropy)]\n",
    "topological_transfomer = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = topological_transfomer.fit_transform(noisy_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well our features can be classified via Support Vector Machines. See we have a small dataset, let's do a number of train/test splits. This can be accomplished via the `cross-validate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(modelSVM,features,labels,cv = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for such a simple model!\n",
    "\n",
    "**Challenge:** Create a better model using some other vectorization scheme for persistence diagrams, different Takens Embedding parameters and/or a different classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
